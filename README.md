# ğŸš€ Proyecto de IngenierÃ­a de Datos End-to-End con Databricks y dbt

Este repositorio contiene una implementaciÃ³n prÃ¡ctica de una arquitectura de ingenierÃ­a de datos moderna de extremo a extremo, utilizando **Databricks**, **dbt (Data Build Tool)** y siguiendo el patrÃ³n de la **Arquitectura Medallion (Bronze, Silver, Gold)**. El proyecto aborda desafÃ­os comunes del mundo real, como la ingesta incremental, transformaciones complejas, control de cambios, construcciÃ³n dinÃ¡mica de modelos dimensionales y exposiciÃ³n de datos para anÃ¡lisis.

---

## ğŸŒ Arquitectura del Proyecto: Medallion al 100%

El pipeline de datos sigue estrictamente la Arquitectura Medallion, que es una de las mejores prÃ¡cticas en entornos de Lakehouse.

**Flujo de Datos:**  
`CSV Files â†’ Bronze Layer (Databricks Autoloader) â†’ Silver Layer (DLT con CDC y validaciones) â†’ Gold Layer (modelo dimensional) â†’ dbt (modelos analÃ­ticos) â†’ SQL Warehouse (BI y consumo)`

### PropÃ³sitos por capa:
- **Bronze:** AlmacÃ©n inmutable de datos brutos.
- **Silver:** Datos limpios, enriquecidos y con CDC.
- **Gold:** Datos listos para anÃ¡lisis (modelo dimensional).

---

## ğŸ› ï¸ ConfiguraciÃ³n y Prerrequisitos

1. **Cuenta de Databricks**: Se recomienda la EdiciÃ³n Gratuita.
2. **Unity Catalog**: Habilitado por defecto para gobernanza.
3. **Volumes**: Para las capas `raw`, `bronze`, `silver`, `gold`.
4. **dbt Cloud**: Para conexiÃ³n y modelado analÃ­tico.

**Estructura del almacenamiento**  
Se usan esquemas como: `workspace.raw`, `workspace.bronze`, etc.  
Los archivos CSV (bookings, flights, customers, airports) se cargan en `workspace.raw.rawvolume.rawdata`.

---

## âš™ï¸ Capas del Pipeline de Datos

### 1. ğŸ—„ï¸ Capa Bronze â€“ Ingesta Incremental Inteligente

- **Objetivo:** Ingestar archivos CSV dinÃ¡micamente.
- **TÃ©cnicas usadas:**
  - Databricks Autoloader + Structured Streaming.
  - Esquema evolutivo (`rescue` o `addNewColumns`).
  - Notebook parametrizable para cada fuente (`src`).
  - OrquestaciÃ³n con Databricks Workflows.
- **Salida:** Tablas Delta en `/Volumes/workspace/bronze/bronzevolume/{src}/data`.

---

### 2. âœ¨ Capa Silver â€“ Limpieza, CDC y Validaciones (Lakeflow / DLT)

- **Objetivo:** Limpieza, transformaciones y control de cambios (SCD Type 1).
- **TÃ©cnicas usadas:**
  - Pipeline unificado DLT (una sola definiciÃ³n por restricciones CE).
  - `@dlt.expect_all_or_drop` para reglas de calidad.
  - `dlt.create_auto_cdc_flow` para manejar CDC.
  - Problemas de streaming resueltos con `ignore_changes`.
- **Salida:** Tablas limpias, enriquecidas y versionadas.

---

### 3. ğŸ¥‡ Capa Gold â€“ Esquema en Estrella con Constructores DinÃ¡micos

- **Objetivo:** Modelo dimensional con tablas de hechos y dimensiones.
- **Constructores creados:**
  - **Dimensiones**: Claves subrogadas con `monotonically_increasing_id()`, manejo de carga incremental y full.
  - **Hechos**: CÃ¡lculo incremental, joins con dimensiones, UPSERT condicional vÃ­a `MERGE`.
- **Errores clave solucionados:**
  - `Cannot perform merges multiple source rows matched...` al incluir `booking_date` como clave compuesta.
- **Salida:** `DimPassengers`, `DimFlights`, `DimAirports`, `FactBookings`.

---

### 4. ğŸ“ˆ ConexiÃ³n con dbt â€“ Modelado AnalÃ­tico y ExposiciÃ³n

- **Objetivo:** Modelado modular con gobernanza y versionado.
- **Pasos implementados:**
  - ConexiÃ³n entre dbt Cloud y Databricks SQL Warehouse.
  - CreaciÃ³n de modelos dbt (`my_first_dbt_model`) usando SQL.
  - MaterializaciÃ³n como `table` o `view` segÃºn necesidad.
  - EjecuciÃ³n con `dbt run`.
- **Resultado:** Modelos analÃ­ticos gobernados listos para herramientas de BI.

---

## ğŸ§  Aprendizajes y Retos Clave Enfrentados

### Limitaciones y desafÃ­os tÃ©cnicos:
- Solo un DLT activo en EdiciÃ³n Gratuita â†’ lÃ³gica consolidada.
- Tiempos de arranque de clusters.
- Errores por rutas incorrectas al crear tablas externas.
- EliminaciÃ³n de `_rescued_data`, manejo de tipos y nulos.

### Dinamismo:
- Notebooks parametrizables para Bronze, Silver y Gold.
- Uso intensivo de f-strings, lÃ³gica condicional, y generaciÃ³n dinÃ¡mica de SQL.

### MERGE en Hechos:
- Se resolviÃ³ el error de mÃºltiples matches asegurando claves compuestas Ãºnicas.

### DepuraciÃ³n:
- Cada error fue una oportunidad para profundizar en el funcionamiento interno de Databricks y dbt.

---

## ğŸ§° Herramientas y TecnologÃ­as Utilizadas

- **Databricks**:
  - Unity Catalog, Volumes, SQL Warehouse
  - Autoloader, Spark Structured Streaming
  - Lakeflow (DLT), Workflows

- **Apache Spark / Delta Lake**

- **dbt Cloud**

- **Lenguajes**:
  - Python / PySpark
  - SQL

---

## ğŸ¯ Â¿Por quÃ© es importante este proyecto?

Este proyecto va mÃ¡s allÃ¡ de un ejemplo:  
Es una **plantilla extensible y realista** para entornos empresariales. Refleja:

- Buenas prÃ¡cticas de arquitectura Lakehouse.
- AutomatizaciÃ³n y versionado.
- Enfoque dinÃ¡mico, escalable y profesional.

---

> _â€œLa ingenierÃ­a de datos no se trata solo de mover datos. Se trata de construir sistemas resilientes, automatizados y gobernados que evolucionen con el negocio.â€_

---

## ğŸ¤– GeneraciÃ³n del README

Este README fue creado con el apoyo de una IA generativa, guiada paso a paso para documentar el proyecto de forma clara y estructurada.

## 