# üöÄ Proyecto de Ingenier√≠a de Datos End-to-End con Databricks y dbt

Este repositorio contiene una implementaci√≥n pr√°ctica de una arquitectura de ingenier√≠a de datos moderna de extremo a extremo, utilizando **Databricks**, **dbt (Data Build Tool)** y siguiendo el patr√≥n de la **Arquitectura Medallion (Bronze, Silver, Gold)**. El proyecto aborda desaf√≠os comunes del mundo real, como la ingesta incremental, transformaciones complejas, control de cambios, construcci√≥n din√°mica de modelos dimensionales y exposici√≥n de datos para an√°lisis.

---

## üåê Arquitectura del Proyecto: Medallion al 100%

El pipeline de datos sigue estrictamente la Arquitectura Medallion, que es una de las mejores pr√°cticas en entornos de Lakehouse.

**Flujo de Datos:**  
`CSV Files ‚Üí Bronze Layer (Databricks Autoloader) ‚Üí Silver Layer (DLT con CDC y validaciones) ‚Üí Gold Layer (modelo dimensional) ‚Üí dbt (modelos anal√≠ticos) ‚Üí SQL Warehouse (BI y consumo)`

### Prop√≥sitos por capa:
- **Bronze:** Almac√©n inmutable de datos brutos.
- **Silver:** Datos limpios, enriquecidos y con CDC.
- **Gold:** Datos listos para an√°lisis (modelo dimensional).

---

## üõ†Ô∏è Configuraci√≥n y Prerrequisitos

1. **Cuenta de Databricks**: Se recomienda la Edici√≥n Gratuita.
2. **Unity Catalog**: Habilitado por defecto para gobernanza.
3. **Volumes**: Para las capas `raw`, `bronze`, `silver`, `gold`.
4. **dbt Cloud**: Para conexi√≥n y modelado anal√≠tico.

**Estructura del almacenamiento**  
Se usan esquemas como: `workspace.raw`, `workspace.bronze`, etc.  
Los archivos CSV (bookings, flights, customers, airports) se cargan en `workspace.raw.rawvolume.rawdata`.

---

## ‚öôÔ∏è Capas del Pipeline de Datos

### 1. üóÑÔ∏è Capa Bronze ‚Äì Ingesta Incremental Inteligente

- **Objetivo:** Ingestar archivos CSV din√°micamente.
- **T√©cnicas usadas:**
  - Databricks Autoloader + Structured Streaming.
  - Esquema evolutivo (`rescue` o `addNewColumns`).
  - Notebook parametrizable para cada fuente (`src`).
  - Orquestaci√≥n con Databricks Workflows.
- **Salida:** Tablas Delta en `/Volumes/workspace/bronze/bronzevolume/{src}/data`.

---

### 2. ‚ú® Capa Silver ‚Äì Limpieza, CDC y Validaciones (Lakeflow / DLT)

- **Objetivo:** Limpieza, transformaciones y control de cambios (SCD Type 1).
- **T√©cnicas usadas:**
  - Pipeline unificado DLT (una sola definici√≥n por restricciones CE).
  - `@dlt.expect_all_or_drop` para reglas de calidad.
  - `dlt.create_auto_cdc_flow` para manejar CDC.
  - Problemas de streaming resueltos con `ignore_changes`.
- **Salida:** Tablas limpias, enriquecidas y versionadas.

---

### 3. ü•á Capa Gold ‚Äì Esquema en Estrella con Constructores Din√°micos

- **Objetivo:** Modelo dimensional con tablas de hechos y dimensiones.
- **Constructores creados:**
  - **Dimensiones**: Claves subrogadas con `monotonically_increasing_id()`, manejo de carga incremental y full.
  - **Hechos**: C√°lculo incremental, joins con dimensiones, UPSERT condicional v√≠a `MERGE`.
- **Errores clave solucionados:**
  - `Cannot perform merges multiple source rows matched...` al incluir `booking_date` como clave compuesta.
- **Salida:** `DimPassengers`, `DimFlights`, `DimAirports`, `FactBookings`.

---

### 4. üìà Conexi√≥n con dbt ‚Äì Modelado Anal√≠tico y Exposici√≥n

- **Objetivo:** Modelado modular con gobernanza y versionado.
- **Pasos implementados:**
  - Conexi√≥n entre dbt Cloud y Databricks SQL Warehouse.
  - Creaci√≥n de modelos dbt (`my_first_dbt_model`) usando SQL.
  - Materializaci√≥n como `table` o `view` seg√∫n necesidad.
  - Ejecuci√≥n con `dbt run`.
- **Resultado:** Modelos anal√≠ticos gobernados listos para herramientas de BI.

---

## üß† Aprendizajes y Retos Clave Enfrentados

### Limitaciones y desaf√≠os t√©cnicos:
- Solo un DLT activo en Edici√≥n Gratuita ‚Üí l√≥gica consolidada.
- Tiempos de arranque de clusters.
- Errores por rutas incorrectas al crear tablas externas.
- Eliminaci√≥n de `_rescued_data`, manejo de tipos y nulos.

### Dinamismo:
- Notebooks parametrizables para Bronze, Silver y Gold.
- Uso intensivo de f-strings, l√≥gica condicional, y generaci√≥n din√°mica de SQL.

### MERGE en Hechos:
- Se resolvi√≥ el error de m√∫ltiples matches asegurando claves compuestas √∫nicas.

### Depuraci√≥n:
- Cada error fue una oportunidad para profundizar en el funcionamiento interno de Databricks y dbt.

---

## üß∞ Herramientas y Tecnolog√≠as Utilizadas

- **Databricks**:
  - Unity Catalog, Volumes, SQL Warehouse
  - Autoloader, Spark Structured Streaming
  - Lakeflow (DLT), Workflows

- **Apache Spark / Delta Lake**

- **dbt Cloud**

- **Lenguajes**:
  - Python / PySpark
  - SQL

---

## üéØ ¬øPor qu√© es importante este proyecto?

Este proyecto va m√°s all√° de un ejemplo:  
Es una **plantilla extensible y realista** para entornos empresariales. Refleja:

- Buenas pr√°cticas de arquitectura Lakehouse.
- Automatizaci√≥n y versionado.
- Enfoque din√°mico, escalable y profesional.

---

> _‚ÄúLa ingenier√≠a de datos no se trata solo de mover datos. Se trata de construir sistemas resilientes, automatizados y gobernados que evolucionen con el negocio.‚Äù_


## ü§ñ Generaci√≥n del README

Este README fue creado con el apoyo de una IA generativa, guiada paso a paso para documentar el proyecto de forma clara y estructurada.

## Agradecimientos

Para finalizar, queremos dar un **cr√©dito especial** a **Ansh Lamba** por su excepcional tutorial de YouTube, que sirvi√≥ de inspiraci√≥n fundamental para este proyecto. Su contenido detallado y de alta calidad fue clave para el desarrollo y los aprendizajes obtenidos. Puedes encontrar su trabajo y conectar con √©l a trav√©s de los siguientes enlaces:

* **Tutorial original de YouTube:** [https://youtu.be/vT7Oeu7WqHg](https://youtu.be/vT7Oeu7WqHg)
* **Repositorio de GitHub:** [https://github.com/anshlambagit/AnshLambaYoutube/tree/main/Databricks%20End%20To%20End%20Project](https://github.com/anshlambagit/AnshLambaYoutube/tree/main/Databricks%20End%20To%20End%20Project)
* **LinkedIn:** [linkedin.com/in/ansh-lamba-793681184](https://www.linkedin.com/in/ansh-lamba-793681184)